#!/usr/bin/env python3
"""
üöÄ INTEGRA√á√ÉO R√ÅPIDA: HuggingFace com seu Interpretador
Cole isto no seu middleware/ia_interpreter.py
"""

# ============ ADICIONAR NO TOPO DO SEU ia_interpreter.py ============

# from typing import Dict, Optional, List
# import os
# 
# try:
#     from transformers import pipeline
#     TRANSFORMERS_AVAILABLE = True
# except ImportError:
#     TRANSFORMERS_AVAILABLE = False
#     print("‚ö†Ô∏è HuggingFace n√£o instalado. Run: pip install transformers torch")


# ============ ADICIONAR NA CLASSE IAInterpreter ============

class IAInterpreterComTransformers:
    """
    Vers√£o melhorada do IAInterpreter com suporte a HuggingFace
    """
    
    def __init__(self):
        # ... seu c√≥digo existente ...
        self.gemini_key = os.getenv('GEMINI_API_KEY')
        self.openai_key = os.getenv('OPENAI_API_KEY')
        self.model = None
        
        # NOVO: Suporte a Transformers
        self.transformer_classifier = None
        self.usar_transformers = TRANSFORMERS_AVAILABLE
        
        if self.usar_transformers:
            self._init_transformers()
    
    def _init_transformers(self):
        """Inicializa modelo HuggingFace"""
        try:
            print("üì¶ Carregando modelo HuggingFace...")
            self.transformer_classifier = pipeline(
                "zero-shot-classification",
                model="facebook/bart-large-mnli"
            )
            print("‚úÖ HuggingFace carregado!")
        except Exception as e:
            print(f"‚ö†Ô∏è Erro ao carregar HuggingFace: {e}")
            self.usar_transformers = False
    
    def interpretar(self, mensagem: str, contexto: dict = None, arquivo_dados: dict = None) -> dict:
        """
        Interpreta com suporte a HuggingFace (MELHORADO)
        """
        mensagem_lower = mensagem.lower().strip()
        
        # Se houver arquivo, processa com contexto
        if arquivo_dados:
            return self._interpretar_com_arquivo(mensagem_lower, arquivo_dados, contexto)
        
        # Primeiro tenta interpreta√ß√£o local (r√°pida)
        resultado_local = self._interpretar_local(mensagem_lower)
        
        # Se encontrou inten√ß√£o com confian√ßa alta, retorna
        if resultado_local.get('intencao') != 'desconhecido' and resultado_local.get('confianca', 0) > 0.8:
            return resultado_local
        
        # NOVO: Se confian√ßa m√©dia/baixa, tenta Transformers
        if self.usar_transformers and resultado_local.get('confianca', 0) < 0.8:
            resultado_tf = self._interpretar_com_transformers(mensagem)
            
            # Se Transformers tem alta confian√ßa, usa
            if resultado_tf.get('confianca', 0) > 0.75:
                return resultado_tf
            
            # Caso contr√°rio, mescla resultados
            if resultado_local.get('intencao') == resultado_tf.get('intencao'):
                # Ambos concordam, combina confian√ßa
                resultado_local['confianca'] = (
                    resultado_local.get('confianca', 0.5) + resultado_tf.get('confianca', 0)
                ) / 2
                resultado_local['metodo'] = 'combinado'
                return resultado_local
        
        # Se tem IA dispon√≠vel, usa como √∫ltimo recurso
        if self.model or self.provider == 'openai':
            return self._interpretar_ia(mensagem, contexto)
        
        # Fallback
        return {
            'intencao': 'conversa',
            'acao': 'responder',
            'parametros': {},
            'confianca': 0.3,
            'resposta_direta': self._resposta_generica(mensagem)
        }
    
    def _interpretar_com_transformers(self, mensagem: str) -> dict:
        """
        Interpreta com HuggingFace Zero-Shot Classification
        """
        if not self.usar_transformers or self.transformer_classifier is None:
            return {'intencao': None, 'confianca': 0.0}
        
        try:
            intencoes = [
                'agenda', 'tarefa', 'lembrete', 'financeiro', 
                'email', 'sistema', 'conversa'
            ]
            
            resultado = self.transformer_classifier(
                mensagem,
                intencoes,
                multi_class=False,
                hypothesis_template="Este texto √© sobre {}."
            )
            
            return {
                'intencao': resultado['labels'][0],
                'acao': self._mapear_acao(resultado['labels'][0]),
                'confianca': float(resultado['scores'][0]),
                'parametros': {},
                'metodo': 'transformers'
            }
        except Exception as e:
            print(f"‚ö†Ô∏è Erro Transformers: {e}")
            return {'intencao': None, 'confianca': 0.0}
    
    def _mapear_acao(self, intencao: str) -> str:
        """Mapeia inten√ß√£o para a√ß√£o padr√£o"""
        mapeamento = {
            'agenda': 'adicionar',
            'tarefa': 'adicionar',
            'lembrete': 'criar',
            'financeiro': 'adicionar_despesa',
            'email': 'buscar',
            'sistema': 'ajuda',
            'conversa': 'responder'
        }
        return mapeamento.get(intencao, 'desconhecido')


# ============ MODO QUICK-START (COPIE E COLE) ============

def quick_test_transformers():
    """Teste r√°pido do HuggingFace"""
    try:
        from transformers import pipeline
        
        print("üöÄ Testando HuggingFace...")
        classifier = pipeline("zero-shot-classification", 
                            model="facebook/bart-large-mnli")
        
        mensagens = [
            "Tenho reuni√£o amanh√£",
            "Preciso comprar leite",
            "Me lembra em 30 minutos",
            "Oi, tudo bem?"
        ]
        
        for msg in mensagens:
            result = classifier(msg, 
                              ['agenda', 'tarefa', 'lembrete', 'conversa'],
                              multi_class=False)
            print(f"‚úÖ '{msg}' ‚Üí {result['labels'][0]} ({result['scores'][0]:.2%})")
        
        print("\n‚úÖ HuggingFace funcionando!")
        
    except ImportError:
        print("‚ùå Instale: pip install transformers torch")


# ============ PERFORMANCE COMPARISON ============

def comparar_metodos():
    """Compara velocidade entre m√©todos"""
    import time
    
    mensagens = [
        "Tenho reuni√£o amanh√£ √†s 14h",
        "Preciso comprar leite",
        "Me lembra em 30 minutos",
        "Gastei 50 reais",
        "Qual meu saldo?",
        "Buscar email de Jo√£o"
    ] * 10  # Repetir para teste de performance
    
    print("\nüìä COMPARA√á√ÉO DE PERFORMANCE\n")
    
    # M√©todo 1: Local (Regex)
    start = time.time()
    for msg in mensagens:
        # Simular interpreta√ß√£o local
        _ = msg.lower().startswith(('tenho', 'preciso', 'me lembra'))
    local_time = time.time() - start
    print(f"Local (Regex):     {local_time:.4f}s - ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê R√ÅPIDO")
    
    # M√©todo 2: HuggingFace
    try:
        from transformers import pipeline
        classifier = pipeline("zero-shot-classification")
        
        start = time.time()
        for msg in mensagens[:6]:  # Menos para demonstra√ß√£o
            _ = classifier(msg, ['agenda', 'tarefa', 'lembrete', 'conversa'])
        tf_time = time.time() - start
        print(f"HuggingFace:       {tf_time:.4f}s - ‚≠ê‚≠ê‚≠ê‚≠ê M√âDIO")
        print(f"  Vantagem: Maior acur√°cia em casos complexos")
        
    except ImportError:
        print("HuggingFace:       (n√£o instalado)")
    
    print(f"\nüí° Recomenda√ß√£o: Use Local para r√°pido, HF para complexo")


# ============ INSTALA√á√ÉO R√ÅPIDA ============

def instalar_dependencias():
    """Instala depend√™ncias necess√°rias"""
    import subprocess
    import sys
    
    print("\nüîß Instalando HuggingFace + PyTorch...")
    
    pacotes = [
        'transformers>=4.30.0',
        'torch>=2.0.0',
    ]
    
    for pacote in pacotes:
        print(f"  Installing {pacote}...")
        subprocess.check_call([sys.executable, '-m', 'pip', 'install', pacote])
    
    print("\n‚úÖ Instala√ß√£o conclu√≠da!")
    print("‚ö†Ô∏è IMPORTANTE: PyTorch √© grande (~500MB)")
    print("Isso vai demorar alguns minutos na primeira vez")


# ============ CONFIGURA√á√ÉO RECOMENDADA ============

CONFIGURACAO_RECOMENDADA = {
    "usar_transformers": True,
    "modelo_transformers": "facebook/bart-large-mnli",  # Melhor acur√°cia
    "limiar_confianca_local": 0.8,  # Se < 80%, tenta Transformers
    "limiar_confianca_tf": 0.75,    # Se < 75%, usa outra coisa
    "cache_resultados": True,       # Cache para performance
    "gpu_disponivel": None          # Auto-detectado
}


# ============ EXEMPLO COMPLETO ============

EXEMPLO_USO = """
# 1. Instalar (primeira vez)
pip install transformers torch

# 2. Usar no seu c√≥digo
from middleware.ia_interpreter import IAInterpreterComTransformers

interpretador = IAInterpreterComTransformers()

# 3. Interpretar
resultado = interpretador.interpretar("Tenho reuni√£o amanh√£ √†s 14h")
print(resultado)
# Output: {
#     'intencao': 'agenda',
#     'acao': 'adicionar', 
#     'confianca': 0.95,
#     'metodo': 'combinado'  # Local + HuggingFace
# }

# 4. Com arquivo
resultado = interpretador.interpretar(
    "Processa esse boleto",
    arquivo_dados={'tipo': 'pdf', 'nome': 'boleto.pdf'}
)
"""


if __name__ == '__main__':
    print("="*60)
    print("üöÄ INTEGRA√á√ÉO HuggingFace com Interpretador")
    print("="*60)
    
    print("\n1Ô∏è‚É£ TEST")
    quick_test_transformers()
    
    print("\n2Ô∏è‚É£ COMPARISON")
    comparar_metodos()
    
    print("\n3Ô∏è‚É£ QUICK START")
    print(EXEMPLO_USO)
    
    print("\n4Ô∏è‚É£ INSTALL")
    # Descomente para instalar:
    # instalar_dependencias()

